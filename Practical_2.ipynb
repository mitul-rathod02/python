{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hello\n"
     ]
    }
   ],
   "source": [
    "print(\"hello\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Collecting scikit-learn\n",
      "  Using cached scikit_learn-1.5.2-cp311-cp311-win_amd64.whl.metadata (13 kB)\n",
      "Requirement already satisfied: numpy>=1.19.5 in c:\\users\\hp\\appdata\\roaming\\python\\python311\\site-packages (from scikit-learn) (1.26.4)\n",
      "Requirement already satisfied: scipy>=1.6.0 in c:\\users\\hp\\appdata\\roaming\\python\\python311\\site-packages (from scikit-learn) (1.14.1)\n",
      "Collecting joblib>=1.2.0 (from scikit-learn)\n",
      "  Using cached joblib-1.4.2-py3-none-any.whl.metadata (5.4 kB)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in c:\\users\\hp\\appdata\\roaming\\python\\python311\\site-packages (from scikit-learn) (3.5.0)\n",
      "Using cached scikit_learn-1.5.2-cp311-cp311-win_amd64.whl (11.0 MB)\n",
      "Using cached joblib-1.4.2-py3-none-any.whl (301 kB)\n",
      "Installing collected packages: joblib, scikit-learn\n",
      "Successfully installed joblib-1.4.2 scikit-learn-1.5.2\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Title: Python (programming language)\n",
      "Summary: Python is a high-level, general-purpose programming language. Its design philosophy emphasizes code readability with the use of significant indentation.\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import json\n",
    "\n",
    "def get_wikipedia_page_summary(page_title):\n",
    "    # Define the Wikipedia API endpoint\n",
    "    url = \"https://en.wikipedia.org/api/rest_v1/page/summary/{}\".format(page_title)\n",
    "    \n",
    "    # Send a GET request to the API\n",
    "    response = requests.get(url)\n",
    "    \n",
    "    # Check if the request was successful\n",
    "    if response.status_code == 200:\n",
    "        # Parse the response JSON\n",
    "        data = response.json()\n",
    "        \n",
    "        # Extract the title and summary\n",
    "        title = data.get('title', 'No title')\n",
    "        summary = data.get('extract', 'No summary available')\n",
    "        \n",
    "        return title, summary\n",
    "    else:\n",
    "        return None, \"Error: Unable to retrieve data\"\n",
    "\n",
    "def main():\n",
    "    # Input: Wikipedia page title\n",
    "    page_title = \"Python_(programming_language)\"  # Example page title\n",
    "    \n",
    "    # Get the page summary\n",
    "    title, summary = get_wikipedia_page_summary(page_title)\n",
    "    \n",
    "    # Print the results\n",
    "    if title:\n",
    "        print(f\"Title: {title}\")\n",
    "        print(f\"Summary: {summary}\")\n",
    "    else:\n",
    "        print(summary)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Title: Data science\n",
      "Summary: \n",
      " Data science is an interdisciplinary academic field[1] that uses statistics, scientific computing, scientific methods, processing, scientific visualization, algorithms and systems to extract or extrapolate knowledge and insights from potentially noisy, structured, or unstructured data.[2]\n",
      " Data science also integrates domain knowledge from the underlying application domain (e.g., natural sciences, information technology, and medicine).[3] Data science is multifaceted and can be described as a science, a research paradigm, a research method, a discipline, a workflow, and a profession.[4]\n",
      " Data science is \"a concept to unify statistics, data analysis, informatics, and their related methods\" to \"understand and analyze actual phenomena\" with data.[5] It uses techniques and theories drawn from many fields within the context of mathematics, statistics, computer science, information science, and domain knowledge.[6] However, data science is different from computer science and information science. Turing Award winner Jim Gray imagined data science as a \"fourth paradigm\" of science (empirical, theoretical, computational, and now data-driven) and asserted that \"everything about science is changing because of the impact of information technology\" and the data deluge.[7][8]\n",
      " A data scientist is a professional who creates programming code and combines it with statistical knowledge to create insights from data.[9]\n",
      " Data science is an interdisciplinary field[10] focused on extracting knowledge from typically large data sets and applying the knowledge and insights from that data to solve problems in a wide range of application domains. The field encompasses preparing data for analysis, formulating data science problems, analyzing data, developing data-driven solutions, and presenting findings to inform high-level decisions in a broad range of application domains. As such, it incorporates skills from computer science, statistics, information science, mathematics, data visualization, information visualization, data sonification, data integration, graphic design, complex systems, communication and business.[11][12] Statistician Nathan Yau, drawing on Ben Fry, also links data science to human–computer interaction: users should be able to intuitively control and explore data.[13][14] In 2015, the American Statistical Association identified database management, statistics and machine learning, and distributed and parallel systems as the three emerging foundational professional communities.[15]\n",
      " Many statisticians, including Nate Silver, have argued that data science is not a new field, but rather another name for statistics.[16] Others argue that data science is distinct from statistics because it focuses on problems and techniques unique to digital data.[17] Vasant Dhar writes that statistics emphasizes quantitative data and description. In contrast, data science deals with quantitative and qualitative data (e.g., from images, text, sensors, transactions, customer information, etc.) and emphasizes prediction and action.[18] Andrew Gelman of Columbia University has described statistics as a non-essential part of data science.[19]\n",
      " Stanford professor David Donoho writes that data science is not distinguished from statistics by the size of datasets or use of computing and that many graduate programs misleadingly advertise their analytics and statistics training as the essence of a data-science program. He describes data science as an applied field growing out of traditional statistics.[20]\n",
      " In 1962, John Tukey described a field he called \"data analysis\", which resembles modern data science.[20] In 1985, in a lecture given to the Chinese Academy of Sciences in Beijing, C. F. Jeff Wu used the term \"data science\" for the first time as an alternative name for statistics.[21] Later, attendees at a 1992 statistics symposium at the University of Montpellier  II acknowledged the emergence of a new discipline focused on data of various origins and forms, combining established concepts and principles of statistics and data analysis with computing.[22][23]\n",
      " The term \"data science\" has been traced back to 1974, when Peter Naur proposed it as an alternative name to computer science.[6] In 1996, the International Federation of Classification Societies became the first conference to specifically feature data science as a topic.[6] However, the definition was still in flux. After the 1985 lecture at the Chinese Academy of Sciences in Beijing, in 1997 C. F. Jeff Wu again suggested that statistics should be renamed data science. He reasoned that a new name would help statistics shed inaccurate stereotypes, such as being synonymous with accounting or limited to describing data.[24] In 1998, Hayashi Chikio argued for data science as a new, interdisciplinary concept, with three aspects: data design, collection, and analysis.[23]\n",
      " During the 1990s, popular terms for the process of finding patterns in datasets (which were increasingly large) included \"knowledge discovery\" and \"data mining\".[6][25]\n",
      " In 2012, technologists Thomas H. Davenport and DJ Patil declared \"Data Scientist: The Sexiest Job of the 21st Century\",[26] a catchphrase that was picked up even by major-city newspapers like the New York Times[27] and the Boston Globe.[28] A decade later, they reaffirmed it, stating that \"the job is more in demand than ever with employers\".[29]\n",
      " The modern conception of data science as an independent discipline is sometimes attributed to William S. Cleveland.[30] In a 2001 paper, he advocated an expansion of statistics beyond theory into technical areas; because this would significantly change the field, it warranted a new name.[25] \"Data science\" became more widely used in the next few years: in 2002, the Committee on Data for Science and Technology launched the Data Science Journal. In 2003, Columbia University launched The Journal of Data Science.[25] In 2014, the American Statistical Association's Section on Statistical Learning and Data Mining changed its name to the Section on Statistical Learning and Data Science, reflecting the ascendant popularity of data science.[31]\n",
      " The professional title of \"data scientist\" has been attributed to DJ Patil and Jeff Hammerbacher in 2008.[32] Though it was used by the National Science Board in their 2005 report \"Long-Lived Digital Data Collections: Enabling Research and Education in the 21st Century\", it referred broadly to any key role in managing a digital data collection.[33]\n",
      " There is still no consensus on the definition of data science, and it is considered by some to be a buzzword.[34] Big data is a related marketing term.[35] Data scientists are responsible for breaking down big data into usable information and creating software and algorithms that help companies and organizations determine optimal operations.[36]\n",
      " Data science and data analysis are both important disciplines in the field of data management and analysis, but they differ in several key ways. While both fields involve working with data, data science is more of an interdisciplinary field that involves the application of statistical, computational, and machine learning methods to extract insights from data and make predictions, while data analysis is more focused on the examination and interpretation of data to identify patterns and trends.[37][38]\n",
      " Data analysis typically involves working with smaller, structured datasets to answer specific questions or solve specific problems. This can involve tasks such as data cleaning, data visualization, and exploratory data analysis to gain insights into the data and develop hypotheses about relationships between variables. Data analysts typically use statistical methods to test these hypotheses and draw conclusions from the data. For example, a data analyst might analyze sales data to identify trends in customer behavior and make recommendations for marketing strategies.[37]\n",
      " Data science, on the other hand, is a more complex and iterative process that involves working with larger, more complex datasets that often require advanced computational and statistical methods to analyze. Data scientists often work with unstructured data such as text or images and use machine learning algorithms to build predictive models and make data-driven decisions. In addition to statistical analysis, data science often involves tasks such as data preprocessing, feature engineering, and model selection. For instance, a data scientist might develop a recommendation system for an e-commerce platform by analyzing user behavior patterns and using machine learning algorithms to predict user preferences.[38][39]\n",
      " While data analysis focuses on extracting insights from existing data, data science goes beyond that by incorporating the development and implementation of predictive models to make informed decisions. Data scientists are often responsible for collecting and cleaning data, selecting appropriate analytical techniques, and deploying models in real-world scenarios. They work at the intersection of mathematics, computer science, and domain expertise to solve complex problems and uncover hidden patterns in large datasets.[38]\n",
      " Despite these differences, data science and data analysis are closely related fields and often require similar skill sets. Both fields require a solid foundation in statistics, programming, and data visualization, as well as the ability to communicate findings effectively to both technical and non-technical audiences. Both fields benefit from critical thinking and domain knowledge, as understanding the context and nuances of the data is essential for accurate analysis and modeling.[37][38]\n",
      " In summary, data analysis and data science are distinct yet interconnected disciplines within the broader field of data management and analysis. Data analysis focuses on extracting insights and drawing conclusions from structured data, while data science involves a more comprehensive approach that combines statistical analysis, computational methods, and machine learning to extract insights, build predictive models, and drive data-driven decision-making. Both fields use data to understand patterns, make informed decisions, and solve complex problems across various domains.\n",
      " Cloud computing can offer access to large amounts of computational power and storage.[40] In big data, where volumes of information are continually generated and processed, these platforms can be used to handle complex and resource-intensive analytical tasks.[41]\n",
      " Some distributed computing frameworks are designed to handle big data workloads. These frameworks can enable data scientists to process and analyze large datasets in parallel, which can reducing processing times.[42]\n",
      " Data science involve collecting, processing, and analyzing data which often including personal and sensitive information. Ethical concerns include potential privacy violations, bias perpetuation, and negative societal impacts [43][44]\n",
      " Machine learning models can amplify existing biases present in training data, leading to discriminatory or unfair outcomes.[45][46]\n",
      "\n",
      "--------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "def scrape_wikipedia_page(url):\n",
    "    try:\n",
    "        # Send a GET request to the URL\n",
    "        response = requests.get(url)\n",
    "        \n",
    "        # Check if the request was successful\n",
    "        if response.status_code == 200:\n",
    "            # Parse the HTML content\n",
    "            soup = BeautifulSoup(response.content, 'html.parser')\n",
    "            \n",
    "            # Extract the title and the first paragraph of the page\n",
    "            title = soup.find('h1', {'id': 'firstHeading'}).text\n",
    "            paragraphs = soup.find_all('p')\n",
    "            summary = ' '.join(paragraph.text for paragraph in paragraphs[:2])  # First 2 paragraphs\n",
    "            \n",
    "            return title, summary\n",
    "        else:\n",
    "            return None, \"Error: Unable to retrieve data\"\n",
    "    except Exception as e:\n",
    "        return None, f\"Exception occurred: {e}\"\n",
    "\n",
    "def main():\n",
    "    # List of Wikipedia page URLs (Sitemap)\n",
    "    urls = [\n",
    "        #\"https://en.wikipedia.org/wiki/Python_(programming_language)\",\n",
    "        \"https://en.wikipedia.org/wiki/Data_science\",\n",
    "        #\"https://en.wikipedia.org/wiki/Web_scraping\"\n",
    "    ]\n",
    "    \n",
    "    # Scrape each URL\n",
    "    for url in urls:\n",
    "        title, summary = scrape_wikipedia_page(url)\n",
    "        \n",
    "        # Print the results\n",
    "        if title:\n",
    "            print(f\"Title: {title}\")\n",
    "            print(f\"Summary: {summary}\")\n",
    "            print(\"-\" * 80)\n",
    "        else:\n",
    "            print(summary)\n",
    "            print(\"-\" * 80)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "content    0\n",
      "dtype: int64\n",
      "0\n",
      "content   NaN\n",
      "dtype: float64\n"
     ]
    }
   ],
   "source": [
    "import requests  # Corrected: 'request' to 'requests'\n",
    "from bs4 import BeautifulSoup  # Corrected: 'BeautifulSoap' to 'BeautifulSoup'\n",
    "import pandas as pd\n",
    "from sklearn.impute import SimpleImputer  # Corrected: 'sklern' to 'sklearn', 'simpleimputer' to 'SimpleImputer'\n",
    "\n",
    "urls = [\"https://en.wikipedia.org/wiki/Data_Science\"]  # Corrected: 'weekipedia' to 'wikipedia'\n",
    "\n",
    "scraped_data = []\n",
    "for url in urls:\n",
    "    response = requests.get(url)  # Corrected: 'request' to 'requests'\n",
    "    soup = BeautifulSoup(response.text, \"html.parser\")  # Corrected: 'BeautifulSoap' to 'BeautifulSoup'\n",
    "    paragraph = soup.find_all(\"p\")\n",
    "    \n",
    "# scraped_data.extend([p.text for p in paragraph])  # Added: Extract text from paragraphs and append to the list\n",
    "data = pd.DataFrame(scraped_data, columns=[\"content\"])  # Corrected: 'column' to 'columns'\n",
    "missing_values = data.isnull().sum()\n",
    "print(missing_values)  # Corrected: 'missing_vulues' to 'missing_values'\n",
    "total_missing = missing_values.sum()  # Corrected: 'missing_values().sum' to 'missing_values.sum()'\n",
    "print(total_missing)\n",
    "miss_perc = (missing_values / len(data)) * 100  # Corrected: 'miss_values' to 'missing_values', added closing parenthesis\n",
    "print(miss_perc)\n",
    "data_cleaned = data.dropna()  # No changes here\n",
    "data_cleaned = data.dropna(axis=1)  # No changes here\n",
    "data_filled = data.fillna(1000)  # No changes here\n",
    "imputer = SimpleImputer(strategy='most_frequent')  # Added: Define the imputer for categorical data\n",
    "data_categorical_impute = data.apply(lambda x: pd.Series(imputer.fit_transform(x.values.reshape(-1, 1)).flatten()) if x.dtype == 'object' else x)\n",
    "# Corrected: Fixed the logic for imputing categorical data, 'x.type' to 'x.dtype', reshaped data for imputer\n",
    "data_categorical_impute = data_categorical_impute.copy()  # No changes here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Title: Data science\n",
      "Summary: \n",
      " Data science is an interdisciplinary academic field[1] that uses statistics, scientific computing, scientific methods, processing, scientific visualization, algorithms and systems to extract or extrapolate knowledge and insights from potentially noisy, structured, or unstructured data.[2]\n",
      " Data science also integrates domain knowledge from the underlying application domain (e.g., natural sciences, information technology, and medicine).[3] Data science is multifaceted and can be described as a science, a research paradigm, a research method, a discipline, a workflow, and a profession.[4]\n",
      " Data science is \"a concept to unify statistics, data analysis, informatics, and their related methods\" to \"understand and analyze actual phenomena\" with data.[5] It uses techniques and theories drawn from many fields within the context of mathematics, statistics, computer science, information science, and domain knowledge.[6] However, data science is different from computer science and information science. Turing Award winner Jim Gray imagined data science as a \"fourth paradigm\" of science (empirical, theoretical, computational, and now data-driven) and asserted that \"everything about science is changing because of the impact of information technology\" and the data deluge.[7][8]\n",
      " A data scientist is a professional who creates programming code and combines it with statistical knowledge to create insights from data.[9]\n",
      " Data science is an interdisciplinary field[10] focused on extracting knowledge from typically large data sets and applying the knowledge and insights from that data to solve problems in a wide range of application domains. The field encompasses preparing data for analysis, formulating data science problems, analyzing data, developing data-driven solutions, and presenting findings to inform high-level decisions in a broad range of application domains. As such, it incorporates skills from computer science, statistics, information science, mathematics, data visualization, information visualization, data sonification, data integration, graphic design, complex systems, communication and business.[11][12] Statistician Nathan Yau, drawing on Ben Fry, also links data science to human–computer interaction: users should be able to intuitively control and explore data.[13][14] In 2015, the American Statistical Association identified database management, statistics and machine learning, and distributed and parallel systems as the three emerging foundational professional communities.[15]\n",
      " Many statisticians, including Nate Silver, have argued that data science is not a new field, but rather another name for statistics.[16] Others argue that data science is distinct from statistics because it focuses on problems and techniques unique to digital data.[17] Vasant Dhar writes that statistics emphasizes quantitative data and description. In contrast, data science deals with quantitative and qualitative data (e.g., from images, text, sensors, transactions, customer information, etc.) and emphasizes prediction and action.[18] Andrew Gelman of Columbia University has described statistics as a non-essential part of data science.[19]\n",
      " Stanford professor David Donoho writes that data science is not distinguished from statistics by the size of datasets or use of computing and that many graduate programs misleadingly advertise their analytics and statistics training as the essence of a data-science program. He describes data science as an applied field growing out of traditional statistics.[20]\n",
      " In 1962, John Tukey described a field he called \"data analysis\", which resembles modern data science.[20] In 1985, in a lecture given to the Chinese Academy of Sciences in Beijing, C. F. Jeff Wu used the term \"data science\" for the first time as an alternative name for statistics.[21] Later, attendees at a 1992 statistics symposium at the University of Montpellier  II acknowledged the emergence of a new discipline focused on data of various origins and forms, combining established concepts and principles of statistics and data analysis with computing.[22][23]\n",
      " The term \"data science\" has been traced back to 1974, when Peter Naur proposed it as an alternative name to computer science.[6] In 1996, the International Federation of Classification Societies became the first conference to specifically feature data science as a topic.[6] However, the definition was still in flux. After the 1985 lecture at the Chinese Academy of Sciences in Beijing, in 1997 C. F. Jeff Wu again suggested that statistics should be renamed data science. He reasoned that a new name would help statistics shed inaccurate stereotypes, such as being synonymous with accounting or limited to describing data.[24] In 1998, Hayashi Chikio argued for data science as a new, interdisciplinary concept, with three aspects: data design, collection, and analysis.[23]\n",
      " During the 1990s, popular terms for the process of finding patterns in datasets (which were increasingly large) included \"knowledge discovery\" and \"data mining\".[6][25]\n",
      " In 2012, technologists Thomas H. Davenport and DJ Patil declared \"Data Scientist: The Sexiest Job of the 21st Century\",[26] a catchphrase that was picked up even by major-city newspapers like the New York Times[27] and the Boston Globe.[28] A decade later, they reaffirmed it, stating that \"the job is more in demand than ever with employers\".[29]\n",
      " The modern conception of data science as an independent discipline is sometimes attributed to William S. Cleveland.[30] In a 2001 paper, he advocated an expansion of statistics beyond theory into technical areas; because this would significantly change the field, it warranted a new name.[25] \"Data science\" became more widely used in the next few years: in 2002, the Committee on Data for Science and Technology launched the Data Science Journal. In 2003, Columbia University launched The Journal of Data Science.[25] In 2014, the American Statistical Association's Section on Statistical Learning and Data Mining changed its name to the Section on Statistical Learning and Data Science, reflecting the ascendant popularity of data science.[31]\n",
      " The professional title of \"data scientist\" has been attributed to DJ Patil and Jeff Hammerbacher in 2008.[32] Though it was used by the National Science Board in their 2005 report \"Long-Lived Digital Data Collections: Enabling Research and Education in the 21st Century\", it referred broadly to any key role in managing a digital data collection.[33]\n",
      " There is still no consensus on the definition of data science, and it is considered by some to be a buzzword.[34] Big data is a related marketing term.[35] Data scientists are responsible for breaking down big data into usable information and creating software and algorithms that help companies and organizations determine optimal operations.[36]\n",
      " Data science and data analysis are both important disciplines in the field of data management and analysis, but they differ in several key ways. While both fields involve working with data, data science is more of an interdisciplinary field that involves the application of statistical, computational, and machine learning methods to extract insights from data and make predictions, while data analysis is more focused on the examination and interpretation of data to identify patterns and trends.[37][38]\n",
      " Data analysis typically involves working with smaller, structured datasets to answer specific questions or solve specific problems. This can involve tasks such as data cleaning, data visualization, and exploratory data analysis to gain insights into the data and develop hypotheses about relationships between variables. Data analysts typically use statistical methods to test these hypotheses and draw conclusions from the data. For example, a data analyst might analyze sales data to identify trends in customer behavior and make recommendations for marketing strategies.[37]\n",
      " Data science, on the other hand, is a more complex and iterative process that involves working with larger, more complex datasets that often require advanced computational and statistical methods to analyze. Data scientists often work with unstructured data such as text or images and use machine learning algorithms to build predictive models and make data-driven decisions. In addition to statistical analysis, data science often involves tasks such as data preprocessing, feature engineering, and model selection. For instance, a data scientist might develop a recommendation system for an e-commerce platform by analyzing user behavior patterns and using machine learning algorithms to predict user preferences.[38][39]\n",
      " While data analysis focuses on extracting insights from existing data, data science goes beyond that by incorporating the development and implementation of predictive models to make informed decisions. Data scientists are often responsible for collecting and cleaning data, selecting appropriate analytical techniques, and deploying models in real-world scenarios. They work at the intersection of mathematics, computer science, and domain expertise to solve complex problems and uncover hidden patterns in large datasets.[38]\n",
      " Despite these differences, data science and data analysis are closely related fields and often require similar skill sets. Both fields require a solid foundation in statistics, programming, and data visualization, as well as the ability to communicate findings effectively to both technical and non-technical audiences. Both fields benefit from critical thinking and domain knowledge, as understanding the context and nuances of the data is essential for accurate analysis and modeling.[37][38]\n",
      " In summary, data analysis and data science are distinct yet interconnected disciplines within the broader field of data management and analysis. Data analysis focuses on extracting insights and drawing conclusions from structured data, while data science involves a more comprehensive approach that combines statistical analysis, computational methods, and machine learning to extract insights, build predictive models, and drive data-driven decision-making. Both fields use data to understand patterns, make informed decisions, and solve complex problems across various domains.\n",
      " Cloud computing can offer access to large amounts of computational power and storage.[40] In big data, where volumes of information are continually generated and processed, these platforms can be used to handle complex and resource-intensive analytical tasks.[41]\n",
      " Some distributed computing frameworks are designed to handle big data workloads. These frameworks can enable data scientists to process and analyze large datasets in parallel, which can reducing processing times.[42]\n",
      " Data science involve collecting, processing, and analyzing data which often including personal and sensitive information. Ethical concerns include potential privacy violations, bias perpetuation, and negative societal impacts [43][44]\n",
      " Machine learning models can amplify existing biases present in training data, leading to discriminatory or unfair outcomes.[45][46]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# Step 1: URL of the Wikipedia page to scrape\n",
    "url = \"https://en.wikipedia.org/wiki/Data_science\"\n",
    "\n",
    "# Step 2: Send a GET request to the URL\n",
    "response = requests.get(url)\n",
    "\n",
    "# Step 3: Check if the request was successful (status code 200)\n",
    "if response.status_code == 200:\n",
    "    # Step 4: Parse the HTML content using BeautifulSoup\n",
    "    soup = BeautifulSoup(response.content, 'html.parser')\n",
    "    \n",
    "    # Step 5: Extract the title (found in the <h1> tag with id 'firstHeading')\n",
    "    title = soup.find('h1', {'id': 'firstHeading'}).text\n",
    "    \n",
    "    # Step 6: Find and extract the first two paragraphs (inside <p> tags)\n",
    "    paragraphs = soup.find_all('p')  # Get all paragraphs\n",
    "    summary = ' '.join(paragraph.text for paragraph in paragraphs[:])  # Combine first two paragraphs\n",
    "    \n",
    "    # Step 7: Print the title and summary\n",
    "    print(f\"Title: {title}\")\n",
    "    print(f\"Summary: {summary}\")\n",
    "else:\n",
    "    print(\"Error: Unable to retrieve data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataFrame:\n",
      "           Title                                            Summary\n",
      "0  Data science  \\n Data science is an interdisciplinary academ...\n",
      "\n",
      "Checking for missing values:\n",
      "Title      0\n",
      "Summary    0\n",
      "dtype: int64\n",
      "\n",
      "Total count of missing values: 0\n",
      "Percentage of missing values: 0.00%\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "\n",
    "# Step 1: URL of the Wikipedia page to scrape\n",
    "url = \"https://en.wikipedia.org/wiki/Data_science\"\n",
    "\n",
    "# Step 2: Send a GET request to the URL\n",
    "response = requests.get(url)\n",
    "\n",
    "# Step 3: Check if the request was successful (status code 200)\n",
    "if response.status_code == 200:\n",
    "    # Step 4: Parse the HTML content using BeautifulSoup\n",
    "    soup = BeautifulSoup(response.content, 'html.parser')\n",
    "    \n",
    "    # Step 5: Extract the title (found in the <h1> tag with id 'firstHeading')\n",
    "    title = soup.find('h1', {'id': 'firstHeading'}).text\n",
    "    \n",
    "    # Step 6: Find and extract **all paragraphs** (inside <p> tags)\n",
    "    paragraphs = soup.find_all('p')  # Get all <p> tags\n",
    "    summary = ' '.join(paragraph.text for paragraph in paragraphs)  # Combine text from all paragraphs\n",
    "    \n",
    "    # Step 7: Create a DataFrame with the title and all paragraphs as summary\n",
    "    data = {'Title': [title], 'Summary': [summary]}\n",
    "    df = pd.DataFrame(data)\n",
    "    \n",
    "    # Step 8: Check for missing values in the DataFrame\n",
    "    print(\"DataFrame:\\n\", df)\n",
    "    print(\"\\nChecking for missing values:\")\n",
    "    print(df.isnull().sum())  # Check for missing values in each column\n",
    "    \n",
    "    # Step 9: Compute the total count of missing values\n",
    "    total_missing_values = df.isnull().sum().sum()  # Total number of missing values\n",
    "    \n",
    "    # Step 10: Compute the percentage of missing values\n",
    "    total_cells = df.size  # Total number of cells in the DataFrame\n",
    "    missing_percentage = (total_missing_values / total_cells) * 100  # Calculate percentage\n",
    "    \n",
    "    # Step 11: Display the total count and percentage of missing values\n",
    "    print(\"\\nTotal count of missing values:\", total_missing_values)\n",
    "    print(f\"Percentage of missing values: {missing_percentage:.2f}%\")\n",
    "else:\n",
    "    print(\"Error: Unable to retrieve data\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataFrame before removing columns with missing values:\n",
      "           Title                                            Summary\n",
      "0  Data science  \\n Data science is an interdisciplinary academ...\n",
      "\n",
      "Checking for missing values:\n",
      "Title      0\n",
      "Summary    0\n",
      "dtype: int64\n",
      "\n",
      "DataFrame after removing columns with missing values:\n",
      "           Title                                            Summary\n",
      "0  Data science  \\n Data science is an interdisciplinary academ...\n",
      "\n",
      "Total count of missing values after removing columns: 0\n",
      "Percentage of missing values after removing columns: 0.00%\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "\n",
    "# Step 1: URL of the Wikipedia page to scrape\n",
    "url = \"https://en.wikipedia.org/wiki/Data_science\"\n",
    "\n",
    "# Step 2: Send a GET request to the URL\n",
    "response = requests.get(url)\n",
    "\n",
    "# Step 3: Check if the request was successful (status code 200)\n",
    "if response.status_code == 200:\n",
    "    # Step 4: Parse the HTML content using BeautifulSoup\n",
    "    soup = BeautifulSoup(response.content, 'html.parser')\n",
    "    \n",
    "    # Step 5: Extract the title (found in the <h1> tag with id 'firstHeading')\n",
    "    title = soup.find('h1', {'id': 'firstHeading'}).text\n",
    "    \n",
    "    # Step 6: Find and extract **all paragraphs** (inside <p> tags)\n",
    "    paragraphs = soup.find_all('p')  # Get all <p> tags\n",
    "    summary = ' '.join(paragraph.text for paragraph in paragraphs)  # Combine text from all paragraphs\n",
    "    \n",
    "    # Step 7: Create a DataFrame with the title and all paragraphs as summary\n",
    "    data = {'Title': [title], 'Summary': [summary]}\n",
    "    df = pd.DataFrame(data)\n",
    "    \n",
    "    # Step 8: Check for missing values in the DataFrame\n",
    "    print(\"DataFrame before removing columns with missing values:\\n\", df)\n",
    "    print(\"\\nChecking for missing values:\")\n",
    "    print(df.isnull().sum())  # Check for missing values in each column\n",
    "    \n",
    "    # Step 9: Remove columns with missing values\n",
    "    df_cleaned_columns = df.dropna(axis=1)  # Remove columns where any value is missing\n",
    "    \n",
    "    # Step 10: Display the DataFrame after removing columns with missing values\n",
    "    print(\"\\nDataFrame after removing columns with missing values:\\n\", df_cleaned_columns)\n",
    "    \n",
    "    # Step 11: Check the total count of missing values after column removal\n",
    "    total_missing_values_cleaned = df_cleaned_columns.isnull().sum().sum()  # Total number of missing values after cleaning\n",
    "    \n",
    "    # Step 12: Compute the percentage of missing values after column removal\n",
    "    total_cells_cleaned = df_cleaned_columns.size  # Total number of cells in the cleaned DataFrame\n",
    "    missing_percentage_cleaned = (total_missing_values_cleaned / total_cells_cleaned) * 100  # Calculate percentage\n",
    "    \n",
    "    # Step 13: Display the total count and percentage of missing values after cleaning\n",
    "    print(\"\\nTotal count of missing values after removing columns:\", total_missing_values_cleaned)\n",
    "    print(f\"Percentage of missing values after removing columns: {missing_percentage_cleaned:.2f}%\")\n",
    "else:\n",
    "    print(\"Error: Unable to retrieve data\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataFrame before removing columns with missing values:\n",
      "           Title                                            Summary\n",
      "0  Data science  \\n Data science is an interdisciplinary academ...\n",
      "\n",
      "Checking for missing values:\n",
      "Title      0\n",
      "Summary    0\n",
      "dtype: int64\n",
      "\n",
      "DataFrame after removing columns with missing values:\n",
      "           Title                                            Summary\n",
      "0  Data science  \\n Data science is an interdisciplinary academ...\n",
      "\n",
      "Total count of missing values after removing columns: 0\n",
      "Percentage of missing values after removing columns: 0.00%\n",
      "\n",
      "DataFrame after populating missing values with a constant value of 1000:\n",
      "           Title                                            Summary\n",
      "0  Data science  \\n Data science is an interdisciplinary academ...\n",
      "\n",
      "DataFrame after replacing missing values with their means:\n",
      "           Title                                            Summary\n",
      "0  Data science  \\n Data science is an interdisciplinary academ...\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "\n",
    "# Step 1: URL of the Wikipedia page to scrape\n",
    "url = \"https://en.wikipedia.org/wiki/Data_science\"\n",
    "\n",
    "# Step 2: Send a GET request to the URL\n",
    "response = requests.get(url)\n",
    "\n",
    "# Step 3: Check if the request was successful (status code 200)\n",
    "if response.status_code == 200:\n",
    "    # Step 4: Parse the HTML content using BeautifulSoup\n",
    "    soup = BeautifulSoup(response.content, 'html.parser')\n",
    "    \n",
    "    # Step 5: Extract the title (found in the <h1> tag with id 'firstHeading')\n",
    "    title = soup.find('h1', {'id': 'firstHeading'}).text\n",
    "    \n",
    "    # Step 6: Find and extract **all paragraphs** (inside <p> tags)\n",
    "    paragraphs = soup.find_all('p')  # Get all <p> tags\n",
    "    summary = ' '.join(paragraph.text for paragraph in paragraphs)  # Combine text from all paragraphs\n",
    "    \n",
    "    # Step 7: Create a DataFrame with the title and all paragraphs as summary\n",
    "    data = {'Title': [title], 'Summary': [summary]}\n",
    "    df = pd.DataFrame(data)\n",
    "    \n",
    "    # Step 8: Check for missing values in the DataFrame\n",
    "    print(\"DataFrame before removing columns with missing values:\\n\", df)\n",
    "    print(\"\\nChecking for missing values:\")\n",
    "    print(df.isnull().sum())  # Check for missing values in each column\n",
    "    \n",
    "    # Step 9: Remove columns with missing values\n",
    "    df_cleaned_columns = df.dropna(axis=1)  # Remove columns where any value is missing\n",
    "    \n",
    "    # Step 10: Display the DataFrame after removing columns with missing values\n",
    "    print(\"\\nDataFrame after removing columns with missing values:\\n\", df_cleaned_columns)\n",
    "    \n",
    "    # Step 11: Check the total count of missing values after column removal\n",
    "    total_missing_values_cleaned = df_cleaned_columns.isnull().sum().sum()  # Total number of missing values after cleaning\n",
    "    \n",
    "    # Step 12: Compute the percentage of missing values after column removal\n",
    "    total_cells_cleaned = df_cleaned_columns.size  # Total number of cells in the cleaned DataFrame\n",
    "    missing_percentage_cleaned = (total_missing_values_cleaned / total_cells_cleaned) * 100  # Calculate percentage\n",
    "    \n",
    "    # Step 13: Display the total count and percentage of missing values after cleaning\n",
    "    print(\"\\nTotal count of missing values after removing columns:\", total_missing_values_cleaned)\n",
    "    print(f\"Percentage of missing values after removing columns: {missing_percentage_cleaned:.2f}%\")\n",
    "    \n",
    "    # Step 14: Populate missing values with a constant value of 1000\n",
    "    df_filled_constant = df.fillna(1000)  # Replace missing values with 1000\n",
    "    \n",
    "    # Display the DataFrame after filling with constant\n",
    "    print(\"\\nDataFrame after populating missing values with a constant value of 1000:\\n\", df_filled_constant)\n",
    "    \n",
    "    # Step 15: Replace missing values with the mean of each column (for quantitative variables)\n",
    "    # Note: This will work only if the DataFrame has numeric columns\n",
    "    df_filled_mean = df.fillna(df.mean(numeric_only=True))  # Replace missing values with the mean of each numeric column\n",
    "    \n",
    "    # Display the DataFrame after replacing missing values with their means\n",
    "    print(\"\\nDataFrame after replacing missing values with their means:\\n\", df_filled_mean)\n",
    "else:\n",
    "    print(\"Error: Unable to retrieve data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataFrame before removing columns with missing values:\n",
      "           Title                                            Summary\n",
      "0  Data science  \\n Data science is an interdisciplinary academ...\n",
      "\n",
      "Checking for missing values:\n",
      "Title      0\n",
      "Summary    0\n",
      "dtype: int64\n",
      "\n",
      "DataFrame after removing columns with missing values:\n",
      "           Title                                            Summary\n",
      "0  Data science  \\n Data science is an interdisciplinary academ...\n",
      "\n",
      "Total count of missing values after removing columns: 0\n",
      "Percentage of missing values after removing columns: 0.00%\n",
      "\n",
      "DataFrame after populating missing values with a constant value of 1000:\n",
      "           Title                                            Summary\n",
      "0  Data science  \\n Data science is an interdisciplinary academ...\n",
      "\n",
      "DataFrame after replacing missing values with their means:\n",
      "           Title                                            Summary\n",
      "0  Data science  \\n Data science is an interdisciplinary academ...\n",
      "\n",
      "DataFrame after imputing missing values in categorical variables:\n",
      "           Title                                            Summary\n",
      "0  Data science  \\n Data science is an interdisciplinary academ...\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "# Step 1: URL of the Wikipedia page to scrape\n",
    "url = \"https://en.wikipedia.org/wiki/Data_science\"\n",
    "\n",
    "# Step 2: Send a GET request to the URL\n",
    "response = requests.get(url)\n",
    "\n",
    "# Step 3: Check if the request was successful (status code 200)\n",
    "if response.status_code == 200:\n",
    "    # Step 4: Parse the HTML content using BeautifulSoup\n",
    "    soup = BeautifulSoup(response.content, 'html.parser')\n",
    "    \n",
    "    # Step 5: Extract the title (found in the <h1> tag with id 'firstHeading')\n",
    "    title = soup.find('h1', {'id': 'firstHeading'}).text\n",
    "    \n",
    "    # Step 6: Find and extract **all paragraphs** (inside <p> tags)\n",
    "    paragraphs = soup.find_all('p')  # Get all <p> tags\n",
    "    summary = ' '.join(paragraph.text for paragraph in paragraphs)  # Combine text from all paragraphs\n",
    "    \n",
    "    # Step 7: Create a DataFrame with the title and all paragraphs as summary\n",
    "    data = {'Title': [title], 'Summary': [summary]}\n",
    "    df = pd.DataFrame(data)\n",
    "    \n",
    "    # Step 8: Check for missing values in the DataFrame\n",
    "    print(\"DataFrame before removing columns with missing values:\\n\", df)\n",
    "    print(\"\\nChecking for missing values:\")\n",
    "    print(df.isnull().sum())  # Check for missing values in each column\n",
    "    \n",
    "    # Step 9: Remove columns with missing values\n",
    "    df_cleaned_columns = df.dropna(axis=1)  # Remove columns where any value is missing\n",
    "    \n",
    "    # Step 10: Display the DataFrame after removing columns with missing values\n",
    "    print(\"\\nDataFrame after removing columns with missing values:\\n\", df_cleaned_columns)\n",
    "    \n",
    "    # Step 11: Check the total count of missing values after column removal\n",
    "    total_missing_values_cleaned = df_cleaned_columns.isnull().sum().sum()  # Total number of missing values after cleaning\n",
    "    \n",
    "    # Step 12: Compute the percentage of missing values after column removal\n",
    "    total_cells_cleaned = df_cleaned_columns.size  # Total number of cells in the cleaned DataFrame\n",
    "    missing_percentage_cleaned = (total_missing_values_cleaned / total_cells_cleaned) * 100  # Calculate percentage\n",
    "    \n",
    "    # Step 13: Display the total count and percentage of missing values after cleaning\n",
    "    print(\"\\nTotal count of missing values after removing columns:\", total_missing_values_cleaned)\n",
    "    print(f\"Percentage of missing values after removing columns: {missing_percentage_cleaned:.2f}%\")\n",
    "    \n",
    "    # Step 14: Populate missing values with a constant value of 1000\n",
    "    df_filled_constant = df.fillna(1000)  # Replace missing values with 1000\n",
    "    \n",
    "    # Display the DataFrame after filling with constant\n",
    "    print(\"\\nDataFrame after populating missing values with a constant value of 1000:\\n\", df_filled_constant)\n",
    "    \n",
    "    # Step 15: Replace missing values with the mean of each column (for quantitative variables)\n",
    "    # Note: This will work only if the DataFrame has numeric columns\n",
    "    df_filled_mean = df.fillna(df.mean(numeric_only=True))  # Replace missing values with the mean of each numeric column\n",
    "    \n",
    "    # Display the DataFrame after replacing missing values with their means\n",
    "    print(\"\\nDataFrame after replacing missing values with their means:\\n\", df_filled_mean)\n",
    "\n",
    "    # Step 16: Impute missing values in categorical variables\n",
    "    # Identify categorical columns (in this case, we only have Title and Summary, both are categorical)\n",
    "    categorical_cols = df.select_dtypes(include=['object']).columns  # Select categorical columns\n",
    "\n",
    "    # Create an imputer for categorical variables\n",
    "    imputer = SimpleImputer(strategy='constant', fill_value='Unknown')  # Use 'Unknown' to fill missing values\n",
    "\n",
    "    # Fit the imputer and transform the categorical columns\n",
    "    df[categorical_cols] = imputer.fit_transform(df[categorical_cols])  # Impute categorical variables\n",
    "\n",
    "    # Display the DataFrame after categorical imputation\n",
    "    print(\"\\nDataFrame after imputing missing values in categorical variables:\\n\", df)\n",
    "\n",
    "else:\n",
    "    print(\"Error: Unable to retrieve data\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello           Title                                         Paragraphs\n",
      "0  Data science  \\n Data science is an interdisciplinary academ...\n",
      "DataFrame before removing columns with missing values:\n",
      "           Title                                         Paragraphs\n",
      "0  Data science  \\n Data science is an interdisciplinary academ...\n",
      "\n",
      "Checking for missing values:\n",
      "Title         0\n",
      "Paragraphs    0\n",
      "dtype: int64\n",
      "\n",
      "DataFrame after removing columns with missing values:\n",
      "           Title                                         Paragraphs\n",
      "0  Data science  \\n Data science is an interdisciplinary academ...\n",
      "\n",
      "Total count of missing values after removing columns: 0\n",
      "Percentage of missing values after removing columns: 0.00%\n",
      "\n",
      "DataFrame after populating missing values with a constant value of 1000:\n",
      "           Title                                         Paragraphs\n",
      "0  Data science  \\n Data science is an interdisciplinary academ...\n",
      "\n",
      "DataFrame after replacing missing values with their means:\n",
      "           Title                                         Paragraphs\n",
      "0  Data science  \\n Data science is an interdisciplinary academ...\n",
      "\n",
      "DataFrame after substituting missing values in categorical variables with initial imputed values:\n",
      "           Title                                         Paragraphs\n",
      "0  Data science  \\n Data science is an interdisciplinary academ...\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "# Step 1: URL of the Wikipedia page to scrape\n",
    "url = \"https://en.wikipedia.org/wiki/Data_science\"\n",
    "\n",
    "# Step 2: Send a GET request to the URL\n",
    "response = requests.get(url)\n",
    "\n",
    "# Step 3: Check if the request was successful (status code 200)\n",
    "if response.status_code == 200:\n",
    "    # Step 4: Parse the HTML content using BeautifulSoup\n",
    "    soup = BeautifulSoup(response.content, 'html.parser')\n",
    "    \n",
    "    # Step 5: Extract the title (found in the <h1> tag with id 'firstHeading')\n",
    "    title = soup.find('h1', {'id': 'firstHeading'}).text\n",
    "    \n",
    "    # Step 6: Find and extract **all paragraphs** (inside <p> tags)\n",
    "    paragraphs = soup.find_all('p')  # Get all <p> tags\n",
    "    paragr = ' '.join(paragraph.text for paragraph in paragraphs)  # Combine text from all paragraphs\n",
    "    \n",
    "    # Step 7: Create a DataFrame with the title and all paragraphs as summary\n",
    "    data = {'Title': [title], 'Paragraphs': [paragr]}\n",
    "    df = pd.DataFrame(data)\n",
    "    \n",
    "    # Step 8: Check for missing values in the DataFrame\n",
    "    print(\"DataFrame before removing columns with missing values:\\n\", df)\n",
    "    print(\"\\nChecking for missing values:\")\n",
    "    print(df.isnull().sum())  # Check for missing values in each column\n",
    "    \n",
    "    # Step 9: Remove columns with missing values\n",
    "    df_cleaned_columns = df.dropna(axis=1)  # Remove columns where any value is missing\n",
    "    \n",
    "    # Step 10: Display the DataFrame after removing columns with missing values\n",
    "    print(\"\\nDataFrame after removing columns with missing values:\\n\", df_cleaned_columns)\n",
    "    \n",
    "    # Step 11: Check the total count of missing values after column removal\n",
    "    total_missing_values_cleaned = df_cleaned_columns.isnull().sum().sum()  # Total number of missing values after cleaning\n",
    "    \n",
    "    # Step 12: Compute the percentage of missing values after column removal\n",
    "    total_cells_cleaned = df_cleaned_columns.size  # Total number of cells in the cleaned DataFrame\n",
    "    missing_percentage_cleaned = (total_missing_values_cleaned / total_cells_cleaned) * 100  # Calculate percentage\n",
    "    \n",
    "    # Step 13: Display the total count and percentage of missing values after cleaning\n",
    "    print(\"\\nTotal count of missing values after removing columns:\", total_missing_values_cleaned)\n",
    "    print(f\"Percentage of missing values after removing columns: {missing_percentage_cleaned:.2f}%\")\n",
    "    \n",
    "    # Step 14: Populate missing values with a constant value of 1000\n",
    "    df_filled_constant = df.fillna(1000)  # Replace missing values with 1000\n",
    "    \n",
    "    # Display the DataFrame after filling with constant\n",
    "    print(\"\\nDataFrame after populating missing values with a constant value of 1000:\\n\", df_filled_constant)\n",
    "    \n",
    "    # Step 15: Replace missing values with the mean of each column (for quantitative variables)\n",
    "    # Note: This will work only if the DataFrame has numeric columns\n",
    "    df_filled_mean = df.fillna(df.mean(numeric_only=True))  # Replace missing values with the mean of each numeric column\n",
    "    \n",
    "    # Display the DataFrame after replacing missing values with their means\n",
    "    print(\"\\nDataFrame after replacing missing values with their means:\\n\", df_filled_mean)\n",
    "\n",
    "    # Step 16: Impute missing values in categorical variables\n",
    "    # Identify categorical columns (in this case, we only have Title and Summary, both are categorical)\n",
    "    categorical_cols = df.select_dtypes(include=['object']).columns  # Select categorical columns\n",
    "\n",
    "    # Create an imputer for categorical variables\n",
    "    imputer = SimpleImputer(strategy='constant', fill_value='Unknown')  # Use 'Unknown' to fill missing values\n",
    "\n",
    "    # Fit the imputer and transform the categorical columns to get initial imputed values\n",
    "    initial_imputed_values = imputer.fit_transform(df[categorical_cols])\n",
    "\n",
    "    # Substitute missing values with the initial imputed values\n",
    "    df[categorical_cols] = initial_imputed_values  # Replace missing values with initial imputed values\n",
    "\n",
    "    # Display the DataFrame after substituting with initial imputed values\n",
    "    print(\"\\nDataFrame after substituting missing values in categorical variables with initial imputed values:\\n\", df)\n",
    "\n",
    "else:\n",
    "    print(\"Error: Unable to retrieve data\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
